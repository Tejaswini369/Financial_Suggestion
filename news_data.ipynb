{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd5d2834-4cc8-4300-90d1-1441d9372a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\kteja\\anaconda3\\envs\\llm_finance\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\kteja\\anaconda3\\envs\\llm_finance\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kteja\\anaconda3\\envs\\llm_finance\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kteja\\anaconda3\\envs\\llm_finance\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kteja\\anaconda3\\envs\\llm_finance\\lib\\site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kteja\\anaconda3\\envs\\llm_finance\\lib\\site-packages (from requests) (2024.7.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kteja\\anaconda3\\envs\\llm_finance\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kteja\\anaconda3\\envs\\llm_finance\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\kteja\\anaconda3\\envs\\llm_finance\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\kteja\\anaconda3\\envs\\llm_finance\\lib\\site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kteja\\anaconda3\\envs\\llm_finance\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pip install requests pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28984ddf-3f76-4edf-b331-025d8de699b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping MarketWatch news...\n",
      "Scraping CNBC news...\n",
      "Scraping completed and data saved to 'multiple_sources_market_news.csv'.\n",
      "                                               title     description  \\\n",
      "0  Earnings playbook: Your guide to the busiest w...  No Description   \n",
      "1  China just expanded its efforts to boost consu...  No Description   \n",
      "2  Apple and Microsoft are among the 'Magnificent...  No Description   \n",
      "3  Stocks soar, Dow closes 650 points higher buoy...  No Description   \n",
      "4  'Rate cut winners': Barclays names global stoc...  No Description   \n",
      "\n",
      "                                                 url publishedAt  \n",
      "0  https://www.cnbc.com/2024/07/28/earnings-playb...     No Date  \n",
      "1  https://www.cnbc.com/2024/07/28/china-just-exp...     No Date  \n",
      "2  https://www.cnbc.com/2024/07/26/most-magnifice...     No Date  \n",
      "3  https://www.cnbc.com/2024/07/25/stock-market-t...     No Date  \n",
      "4  https://www.cnbc.com/2024/07/29/rate-cut-winne...     No Date  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def scrape_marketwatch_news():\n",
    "    url = \"https://www.marketwatch.com/latest-news\"\n",
    "    response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    articles_section = soup.find_all('div', {'class': 'article__content'})\n",
    "    news_list = []\n",
    "\n",
    "    for article in articles_section:\n",
    "        try:\n",
    "            title_tag = article.find('a')\n",
    "            title = title_tag.text.strip() if title_tag else \"No Title\"\n",
    "            url = title_tag['href'] if title_tag else None\n",
    "            description = article.find('p').text.strip() if article.find('p') else \"No Description\"\n",
    "            published_at = article.find('time')['datetime'] if article.find('time') else \"No Date\"\n",
    "            \n",
    "            news_list.append({\n",
    "                'title': title,\n",
    "                'description': description,\n",
    "                'url': url,\n",
    "                'publishedAt': published_at\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing article: {e}\")\n",
    "\n",
    "    return news_list\n",
    "\n",
    "def scrape_cnbc_news():\n",
    "    url = \"https://www.cnbc.com/world/?region=world\"\n",
    "    response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    articles_section = soup.find_all('div', {'class': 'Card-standardBreakerCard'})\n",
    "    news_list = []\n",
    "\n",
    "    for article in articles_section:\n",
    "        try:\n",
    "            title_tag = article.find('a', {'class': 'Card-title'})\n",
    "            title = title_tag.text.strip() if title_tag else \"No Title\"\n",
    "            url = title_tag['href'] if title_tag else None\n",
    "            description = article.find('div', {'class': 'Card-description'}).text.strip() if article.find('div', {'class': 'Card-description'}) else \"No Description\"\n",
    "            published_at = article.find('time')['datetime'] if article.find('time') else \"No Date\"\n",
    "            \n",
    "            news_list.append({\n",
    "                'title': title,\n",
    "                'description': description,\n",
    "                'url': url,\n",
    "                'publishedAt': published_at\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing article: {e}\")\n",
    "\n",
    "    return news_list\n",
    "\n",
    "# Scrape news from multiple sources\n",
    "all_news = []\n",
    "\n",
    "print(\"Scraping MarketWatch news...\")\n",
    "all_news.extend(scrape_marketwatch_news())\n",
    "time.sleep(2)\n",
    "\n",
    "print(\"Scraping CNBC news...\")\n",
    "all_news.extend(scrape_cnbc_news())\n",
    "time.sleep(2)\n",
    "\n",
    "# Convert to DataFrame and save to CSV\n",
    "if all_news:\n",
    "    news_df = pd.DataFrame(all_news)\n",
    "    news_df.to_csv('multiple_sources_market_news.csv', index=False)\n",
    "    print(\"Scraping completed and data saved to 'multiple_sources_market_news.csv'.\")\n",
    "else:\n",
    "    print(\"No news articles found.\")\n",
    "\n",
    "# Optional: Print the first few rows of the DataFrame to verify\n",
    "if all_news:\n",
    "    print(pd.DataFrame(all_news).head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a4fd7b-8c14-4c4c-bb33-986147dcd0b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
